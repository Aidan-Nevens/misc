---
title: "Final Project (Kickstarter)"
author: "Aidan Nevens"
date: "11/21/2020"
output: html_document
---
# Kickstarter: Predicting Success {.tabset .tabset-fade .tabset-pills}

## About the Data

### Dataset Background

```{r library, message=FALSE}
library(tidyverse)
library(readxl)
library(MASS)
library(AER)
library(caret)
library(survival)
library(lmtest)
library(DT)
library(grid)
library(leaps)
library(class)
library(DAAG)
library(neuralnet)
library(nnet)
library(reshape)
```

This dataset centers around the crowd-funding web service ["Kickstarter"](https://www.kickstarter.com/?ref=nav). Kickstarter is a crowd-funding web application with a mission to "help bring creative projects to life". The website allows individuals to back projects that they may find interesting. Parties make their pitches to backers for their new ideas, and set a goal and a deadline. If the crowd funding campaigns are successful the parties responsible for the idea received any funds raised and can work towards bringing their visions to the market. Since 2009, 19 million people have backed kickstarter projects, and over $5 billion dollars have been pledged, funding 192,000 projects. 

The dataset was created by Mikaël Mouillé, a kickstarter enthusiast, and was last updated in 2018. The data itself is actually sourced from the [Kickstarter platform](https://www.kickstarter.com/?ref=nav). Within this dataset each observation consists of a past kickstarter project. With over 375,000 observations (Note I'll only be looking at 175,000 of these observations) this a large dataset that can yield some important insights on how successful a project might be given certain characteristics.

### Contents of the Dataset

* **ID:** The ID is a unique 10 digit number generated by Kickstarter and assigned to a kickstarter project.

* **Name:** The "Name" variable is quite simply the name of the project. 

* **Subcategory**: In this dataset this column was originally called "category". I decided to change the name of this to subcategory as there was another category variable that functioned as the main_category. Subcategory features more specifics about the project, for example if the product has to do with technology is it a gadget, software, or something else? There are 159 different subcategories.

* **Main Category**: In this dataset *main category*, compared to *subcategory* is broad way of classifying projects. For example, rather than specifying if given project has to do with gadgets or software, main category will classify the project broadly as "technology". There are 15 different main categories. Main categories include publishing, film and video, music, food, design, crafts, games, comics, fashion, theater, art, photography, technology, dance, and journalism.

* **Currency**: Currency specifies which currency the money for the project was raised in. Currencies are denoted by their abbreviations. There are 14 different currencies listed here.

* **Deadline**: Deadline denotes the year, month, and date that the funding period of the project will end. Meaning by the deadline, the funding goal would need to be hit. 

* **Goal**: The variable *goal* represents the funding goal for the project. It should be noted that the goal variable is listed in the respective currency for the project. For the purposes of making comparisons, we'll be using the *usd_goal_real* (see below). 

* **Launched**: The variable *launched* represents the date that the kickstarter project was initially launched, meaning the date funding started. This originally included the year, month, date, and time that a project was launched. For the analysis purposes I'll be omitting the time in this variable, as deadline does not include the time of day. 

* **Pledged**: The variable *pledged* represents the total amount that was pledged to the project. Once it should be noted that similarly to *goal*, pledged is listed in the respective currency for the project. With that being said I will be using *usd_pledged_real*(see below) for the purposes of making comparisons. 

* **Status**: Originally named *state*, I changed the name of this variable to *status* to avoid any possible confusion (since we do have a country variable as well). *Status* specifies if a kickstarter project was canceled, had failed, live, suspended, undefined, or was successful. Note I created an additional variable which denotes both project cancellations and failures with a 0 and project success as a 1. For the purposes of creating a model further down the line, I'll be treating cancellations and suspensions as failures, and omitting undefined and live values.

* **Backers**: The variable *backers* lists the number of backers (individuals that gave money) for each kickstarter project.

* **Country**: The variable *Country* specifies which country the kickstarter project was launched from. Countries are denoted by their two-letter abbreviations. Within this dataset, there are 23 different countries.

* **USD Pledged**: This variable denotes the total amount pledged in US Dollars to a kickstarter project. 

* **USD Pledged Real**: This variable denotes the total amount pleged in Real US Dollars (Adjusted for any inflation) to a kickstarter project.

### Data Organization

As previously stated, I'll be defining *canceled* and *suspended* kickstarters as failed kickstarter projects. Additionally, I'll be omitting any *undefined* or *live* kickstarter projects as these do not necessarily tie in with the objective of this research, and we can't definitively determine if live kickstarters are successes or failures. 

```{r dataset, Message = FALSE, echo = FALSE}
Data_2018<- read_csv("2018Data.csv")
names(Data_2018) <- c("id", "name", "subcategory", "main_category", "currency", "deadline", "goal", "launched", "pledged", "status", "backers", "country", "usd_pledged", "usd_pleged_real", "usd_goal_real")
Duration <- read_csv("Duration.csv")

Data_2018 <- left_join(Data_2018, Duration,
                         by = "id")


Kickstarter1 <- filter(Data_2018, status == "canceled")
Kickstarter2 <- filter(Data_2018, status == "suspended")
Kickstarter3 <- filter(Data_2018, status == "failed")
Kickstarter4 <- filter(Data_2018, status == "successful")

Kickstarter <- rbind(Kickstarter1, Kickstarter2)
Kickstarter <- rbind(Kickstarter, Kickstarter3)
Kickstarter <- rbind(Kickstarter, Kickstarter4)
rm("Kickstarter1", "Kickstarter2", "Kickstarter3", "Kickstarter4")

Kickstarter$status <- gsub("canceled", "failed", Kickstarter$status)
Kickstarter$status <- gsub("suspended", "failed", Kickstarter$status)
Kickstarter$status <- gsub("failed", "Failed", Kickstarter$status)
Kickstarter$status <- gsub("successful", "Successful", Kickstarter$status)


```

After filtering the data to the following status criteria, our dataset is now comprised of 372,300 observations. This is still plenty of observations to work with, and now is just comprised of statuses we're interested in!

### Dataset Sample

I wanted to include a sample of the dataset used. Below you'll see a small selection of observations from the dataset. Because the dataset consists of 15 different variables, I've broken the dataset into two different tables to reduce side-scrolling. 

```{r DatasetSamp, echo = FALSE}
KickstarterSamp <- Kickstarter[100000:100100, 1:7]
KickstarterSamp2 <- Kickstarter[100000:100100, 8:15]

datatable(KickstarterSamp)
datatable(KickstarterSamp2)
rm("KickstarterSamp", "KickstarterSamp2")
```

### **<a href="#top">Back to top</a>**



## Research Goals

### Breaking Down Kickstarter

During my graphical analysis, I want to gain some insight on how different projects on kickstarter fare. By making some basic observations via graphical analysis I can begin planning which variables I may use to create my predictive models.  

First I'll see just how many projects within the dataset were successful or failed. This will give me a general idea of how difficult it may be to get funding from kickstarter. Additionally I'll plot each main category against success or failure to see if a certain category experiences higher rates of success. 

### Creating Models

There are two types of models I want to decide between. For one type of model, I'll be looking into creating a predictive model that yields predictions on whether a kickstarter campaign will succeed or fail. For another model, I'll be trying to create a predictive model that seeks to predict how much will be pledged based on a few selected variables. **I'll be selecting whichever model seems like it has a better selection of variables to choose from and can provide better predictions.**


**Note**: I'll try to validate each model and ensure I'm choosing the best fit model, however I understand that the model may not be as accurate as I may hope it is. At the end of the day, since this is an experiment its entirely possible that we end up with a lower than anticipated prediction accuracy. If we were to look at this from an investment standpoint, this is completely reasonable to assume, as start-up projects are considered high-risk ventures. **What should be noted is this project is intended to show that some basic factors may or may not be influential in just how successful a project may be**. It may be the case that certain categories typically gain more traction on the platform than others, or certain funding goals may lead to more backers or less backers. 

It may also be the case that successful and failed projects on kickstarter are much easier to predict than previously anticipated. If this were case, the goal of the research project would still remain the same!


### **<a href="#top">Back to top</a>**


## Graphical Analysis

### Understanding Key Kickstarter Trends

Before getting into some deeper graphical analysis, it's important to understand some of the key trends with kickstarter projects. To start, within our dataset we should ask the following: 

* What proportion of kickstarter projects failed? 

* How many kickstar projects succeeded? 

I'll do this by breaking up our main categories into groups (I'll be doing 3 groups of 5), and measuring what proportion of projects succeeded, and what proportion of projects failed. This can give us a general idea of trends within our kickstarter data across categories.

We can also do a histogram that plots the distribution of the goals in USD across kickstarter projects. Is the data skewed right or left, meaning do goals tend to be set a lower dollar ranges, higher dollar ranges, or is there some sort of approximately normal distribution to kickstarter goals? We'll be displaying all of these key trends within this subsection.

#### Success, Failure, and Cancellation

```{r PropSuccess, echo = FALSE}

grob1 = grobTree(textGrob("238,344", x=0.225,  y=0.665, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob2 = grobTree(textGrob("133,956", x=0.68,  y=0.415, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))



ggplot(Kickstarter, aes(x = status)) + 
  geom_bar(aes(y = (..count..)/sum(..count..)), fill = c("sienna2", "deepskyblue4"), width = .5) +
  annotation_custom(grob1)+
  annotation_custom(grob2)+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  labs(x="Project Status", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

#238,344 Fail, 133,956 Success (I got this from round a stat count earlier)

rm("grob1", "grob2")
```

**Findings**

As is evident from the plot (and frankly expected), a majority of projects on kickstarter fail to reach their funding goals. However, I should note that the percentage of failure is a little lower than I had expected. With approximately 64% of projects in the sample failing and 36% succeeding, I was pleasantly surprised that the proportion of successful projects even surpassed 25%. This is a great indicator for prospective entrepreneurs and even the team at kickstarter. Essentially 1 in 3 projects do end up meeting their funding goals. 

Now that we have a general idea of the proportions of successful and failed kickstarter projects across all categories, we can dig a bit deeper. From here, we can begin examining the proportion of succesful and failed projects for each category.

#### Breaking Down Successes and Failures by Category

Now that we have an overall picture of how many projects succeed and how many fail, I felt it was only reasonable to further break down this analysis into categories. I wanted answer the following questions:

* Are there specific categories that perform well? 

* Are the specific categories that do not? 

As discussed in the previous section, to improve readability, I wanted to break my 15 categories up into three different groups, and plot each group. I'll be running a similar plot which measures the proportion of failed projects and successful projects, but this time these proportions will be grouped by the project categories. 

##### Category Group 1

Before plotting the three groups of categories we'll need to create some new datasets to plot said groups. I'll essentially create 5 new data frames by filtering by category and then simply row bind the data frames to create one group of 5 categories. From there I can plot the group, illustrating the proportion of success and failures by project category. The process is the same for future group plots, so I'll only include the relevant code for this first plot.

```{r CategorySuccess1}
Group1a <- filter(Kickstarter, main_category == "Film & Video")
Group1b <- filter(Kickstarter, main_category == "Design")
Group1c <- filter(Kickstarter, main_category == "Technology")
Group1d <- filter(Kickstarter, main_category == "Games")
Group1e <- filter(Kickstarter, main_category == "Food")

Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)
rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e")

Plot <- ggplot(Group1)+ 
  geom_bar(aes(main_category, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Category")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())
Plot
rm("Group1")
```

**Findings**

From this first group there are two categories which seemed to deviate considerably from our overall success-failure rate of approximately 36% success and 64% failure found earlier. Just by observing our plotted data we can see that the main categories of *food* and *technology* seem to have a failure rate of over 70%. This is surprising, as I expected *technology* to have a higher rate of success. When selecting variables in the future this plot alone is a promising sign that project categories may have some influence on if a project will succeed or fail on the kickstarter platform.


##### Category Group 2

Now lets plot the second group of project categories:

```{r CategorySuccess2, echo = FALSE}
Group2a <- filter(Kickstarter, main_category == "Music")
Group2b <- filter(Kickstarter, main_category == "Photography")
Group2c <- filter(Kickstarter, main_category == "Publishing")
Group2d <- filter(Kickstarter, main_category == "Art")
Group2e <- filter(Kickstarter, main_category == "Fashion")

Group2 <- rbind(Group2a, Group2b)
Group2 <- rbind(Group2, Group2c)
Group2 <- rbind(Group2, Group2d)
Group2 <- rbind(Group2, Group2e)
rm("Group2a", "Group2b", "Group2c", "Group2d", "Group2e")

Plot2 <- ggplot(Group2)+ 
  geom_bar(aes(main_category, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Category")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())
Plot2
rm("Group2")
```

**Findings**

In this second group a few thing stick out. For one, *art* has a small but noticeable deviation from our overall success-failure rate. *Art* has a slightly higher rate of success (~40%) than our overall rate (~36%). *Fashion* projects on kickstarter within our sample had a low rate of success (~25%). *Music* had a surprisingly high rate of success on kickstarter. Almost 50% of *music* projects on kickstarter seem to succeed. *Photography* and *publishing* have a slightly lower rate of success and failure to our overall percentages. Like with the findings in group 1, the findings from this second plot seem to reinforce the notion that category may have some bearing on the chances a project will succeed or fail.

##### Category Group 3

Now lets move on to the plot for our final group of project categories:

```{r CategorySuccess3, echo = FALSE}
Group3a <- filter(Kickstarter, main_category == "Theater")
Group3b <- filter(Kickstarter, main_category == "Crafts")
Group3c <- filter(Kickstarter, main_category == "Journalism")
Group3d <- filter(Kickstarter, main_category == "Comics")
Group3e <- filter(Kickstarter, main_category == "Dance")

Group3 <- rbind(Group3a, Group3b)
Group3 <- rbind(Group3, Group3c)
Group3 <- rbind(Group3, Group3d)
Group3 <- rbind(Group3, Group3e)
rm("Group3a", "Group3b", "Group3c", "Group3d", "Group3e")
Plot3 <- ggplot(Group3)+ 
  geom_bar(aes(main_category, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Category")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

Plot3
rm("Group3")
```

**Findings**

This group perhaps was the most surprising group of the three. Previously, all of our categories had higher failure rates than success rates. In this category group there is a plethora of unexpected insights that should be taken into serious consideration when creating models in the future. To start *comics, dance*, and *theater* had a **higher rate of success** than compared to failure. *Dance* had the highest rate of success within the sample followed by *theater* and *comics*. Additionally, *crafts* had a failure rate of almost 80% along with *journalism*. We'll come back to these plots again in the variable selection section of our research but so far it's evident that our findings indicate that project category does have some effect on success rates.

### Project Goal Breakdowns

For this next section, I wanted to briefly go over some of the trends when it comes to project goals. First, I wanted to create a histogram of goals in real USD across categories. After getting a general sense of how goals differ, I wanted to create a bar graph which shows the average project goals across the main categories. 

#### Overall Distribution of Project Goals

**A Brief Note**:

It should be noted that I'm essentially omitting values past 200,000 from this histogram as they were outliers that caused issues when trying to plot a "readable" histogram. Therefore, I limited the kickstarter goals displayed to 200,000, as there were only a few projects that actually went above this goal. 

```{r histogram1, warning=FALSE, echo = FALSE}
ggplot(Kickstarter, aes(usd_goal_real)) +
  geom_histogram(bins = 35, fill = "deepskyblue4", color = "white")+
  scale_x_continuous(limits=c(0,200000), breaks = seq(0,200000, 20000))+
  labs(x="Project Goal in Real USD", y="Count",
       title="Kickstarter:Distribution of Project Goals ")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())
                     
```

**Findings**

As is evident from the plot a majority of kickstarter campaigns fall below the 20,000 USD mark. In fact, it should be noted that only a few projects have goals above the 100,000 USD mark. To answer a question we posed earlier, it would seem that the distribution of project goals on kickstarter is skewed pretty heavily to the right.

Based on the findings of this first plot, I wanted to observe the distribution of projects between 0 USD and 20,000 USD. Therefore, the following plot will further limit the x axis values to display goals up to 20,000 USD.

```{r histogram2, warning=FALSE, echo = FALSE}
ggplot(Kickstarter, aes(usd_goal_real)) +
  geom_histogram(bins = 20, fill = "deepskyblue4", color = "white")+
  scale_x_continuous(limits=c(0,20000), breaks = seq(0,20000, 2000))+
  labs(x="Project Goal in Real USD", y="Count",
       title="Kickstarter:Distribution of Project Goals ")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())
                     
```

**Additional Findings**

Upon further inspection within the 20,000 USD project goal range, it would seem that many of the kickstarter projects have goals under 6,000 USD. For potential start-up ventures, these do seem to be lower budget projects. Initially, I believed a lot of kickstarter projects had higher budgets (budgets > $10,000), however the following plots have worked to shift my previous conception. Even after zooming in, the shape remains pretty much the same within this narrowed goal range and is still heavily skewed to the right.

#### Project Goals 

For the final part of this section, I just wanted to get a sense of the average goals in USD across categories. This would tell me if some categories typically have higher goals on average. Once again, I'll be breaking up my data into three groups and will discuss my findings for each plot.

**An important note:**

Within each category there are often projects with goals above $200,000. This could raise the dollar value of mean, however, after surveying the data it's apparent that this would inflate the dollar value across all categories. 

Before plotting the average goals by category I needed to create a new data frame. To do this I created a function that calculated the means of each category, then I created two lists (one for category and one for average goal is USD) and combined them. I wanted to display the relevant code used to create this new data frame below.

```{r AverageGoal1}
# Function that calculates average for each category
AvgGoal <- function(category){
  Group <- filter(Kickstarter, main_category == category)
  mean(Group$usd_goal_real)
}

#Excuting the function across all 15 categories
Film<- AvgGoal("Film & Video")
Design<- AvgGoal("Design")
Technology<- AvgGoal("Technology")
Games<- AvgGoal("Games")
Food<- AvgGoal("Food")
Music<- AvgGoal("Music")
Photography<- AvgGoal("Photography")
Publishing<- AvgGoal("Publishing")
Art<- AvgGoal("Art")
Fashion<- AvgGoal("Fashion")
Theater<- AvgGoal("Theater")
Crafts<- AvgGoal("Crafts")
Journalism<- AvgGoal("Journalism")
Comics<- AvgGoal("Comics")
Dance<- AvgGoal("Dance")

#Concatenating the averages and category names to create a new dataframe of average goals by category.

Average <- c(Film , Design, Technology, Games, Food, Music, Photography, Publishing, Art, Fashion, Theater, Crafts, Journalism, Comics, Dance)
Category <- c("Film" , "Design", "Technology", "Games", "Food", "Music", "Photography", "Publishing", "Art", "Fashion", "Theater", "Crafts", "Journalism", "Comics", "Dance")
AverageGoal <- data.frame(Category, Average)

#To improve processing speeds in R we'll remove objects from our global environment that are no longer necessary.

rm("Film" , "Design", "Technology", "Games", "Food", "Music", "Photography", "Publishing", "Art", "Fashion", "Theater", "Crafts", "Journalism", "Comics", "Dance", "Average", "Category")
```

Once the data frame is created I decided to break the categories into three groups just as I had done before and then proceeded to plot each group. 

##### Group 1

Lets start by plotting the averages of our first group:

```{r Group1avg, echo = FALSE}

Group1 <- AverageGoal[1:5,]

grob = grobTree(textGrob("42,046", x=0.09,  y=0.45, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob1 = grobTree(textGrob("82,393", x=0.28,  y=0.825, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob2 = grobTree(textGrob("49,228", x=0.475,  y=0.52, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob3 = grobTree(textGrob("45,132", x= 0.67,  y= 0.47, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob4 = grobTree(textGrob("102,719", x= 0.859,  y= 0.98, hjust=0,
  gp=gpar(col="grey55", fontsize=12, fontface="bold")))

ggplot(Group1, aes(Category, y = Average, fill = Category))+ 
  geom_bar(stat='identity')+
  annotation_custom(grob)+
    annotation_custom(grob1)+
  annotation_custom(grob2)+
    annotation_custom(grob3)+
    annotation_custom(grob4)+
    labs(x="Project Category", y="Average Project Goal (USD)",
       title="Kickstarter: Average Project Goal by Category (USD)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
    theme(panel.background = element_blank())

rm("grob", "grob1", "grob2", "grob3", "grob4")
  
```
 
 **Findings**
 
 As is evident in the plot, within the sample the categories film and technology had project goals that averaged above $75,000. Keeping this in mind, it should be noted that it was found earlier that the success-rate for film projects was closer to the overall success rate across all categories. Meanwhile, we also found that *technology* had a considerably lower success-rate in comparison to the overall success rate across all categories (36%). Additionally, another finding I'd like to note is that the category *games* had a similar success rate to film, however, projects under the *games* category on average costed approximately 25,000 USD less than film projects. 
 
##### Group 2

Lets now plot the average goals in USD for group 2:

```{r Group2avg, echo = FALSE}

Group2 <- AverageGoal[6:10,]

grob = grobTree(textGrob("39,363", x=0.09,  y=0.98, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob1 = grobTree(textGrob("22,449", x=0.28,  y=0.6, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob2 = grobTree(textGrob("15,780", x=0.475,  y=0.45, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob3 = grobTree(textGrob("12,278", x= 0.67,  y= 0.355, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob4 = grobTree(textGrob("25,136", x= 0.859,  y= 0.66, hjust=0,
  gp=gpar(col="grey55", fontsize=12, fontface="bold")))

ggplot(Group2, aes(Category, y = Average, fill = Category))+ 
  geom_bar(stat='identity')+
   annotation_custom(grob)+
    annotation_custom(grob1)+
  annotation_custom(grob2)+
    annotation_custom(grob3)+
    annotation_custom(grob4)+
    labs(x="Project Category", y="Average Project Goal (USD)",
       title="Kickstarter: Average Project Goal by Category (USD)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
    theme(panel.background = element_blank())

rm("grob", "grob1", "grob2", "grob3", "grob4", "Group2")
  
``` 

**Findings**

A notable feature in the second group of categories is that unlikethe first group, none of the categories had goals in real USD that surpassed 50,000. Furthermore, it was found that both Music and Photography had an average project goal that was less than 20,000 USD. Fashion also had one of the lowest average goals we've seen thus far with a average goal that was slightly above 20,000. After viewing the first and second plot it's becoming clear that there is a lot of variation in goals across different categories. 


##### Group 3

Lets now plot our final group just as we did with the previous groups:

```{r Group3avg, echo = FALSE}

Group3 <- AverageGoal[11:15,]

grob = grobTree(textGrob("19,676", x=0.09,  y=0.29, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob1 = grobTree(textGrob("10,451", x=0.28,  y=0.19, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob2 = grobTree(textGrob("9,584", x=0.475,  y=0.18, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob3 = grobTree(textGrob("86,477", x= 0.67,  y= 0.98, hjust=0,
  gp=gpar(col="grey55", fontsize=13, fontface="bold")))
grob4 = grobTree(textGrob("27,147", x= 0.859,  y= 0.37, hjust=0,
  gp=gpar(col="grey55", fontsize=12, fontface="bold")))



ggplot(Group3, aes(Category, y = Average, fill = Category))+ 
  geom_bar(stat='identity')+
      annotation_custom(grob)+
    annotation_custom(grob1)+
  annotation_custom(grob2)+
    annotation_custom(grob3)+
    annotation_custom(grob4)+
  scale_y_continuous(breaks = seq(0,80000, 10000))+
    labs(x="Project Category", y="Average Project Goal (USD)",
       title="Kickstarter: Average Project Goal by Category (USD)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
    theme(panel.background = element_blank())

rm("grob", "grob1", "grob2", "grob3", "grob4", "Group3", "AverageGoal")
``` 

**Findings**

Within our last group of categories lies some more interesting findings. Both crafts and dance had average project goals that were around 10,000 USD. In fact, Dance had a average goal in USD that was slightly below 10,000. Additionally it should be noted that journalism did have a considerably higher average goal in USD compared to other categories (86,477). Across all our groups, it has become evident that goals vary across categories. We'll run some more plots of project goals in USD, only next time we'll be plotting it against whether or not a project succeeds and against how much is pledged to determine if it may influence either dependent variable.

#### Considerations When Selecting Model Variables

It should be noted that some categories experience higher rates of success compared to other categories. Additionally, within the sample it was found that a few of the categories actually had a higher rate of success than failure. This is a sign that the category of a project may have an effect on the probability of success or failure. 

Along with our preliminary findings regarding variations in success rates across categories, it was also found that there was considerable variation across categories in terms of the average goals in real USD. Some categories had averages that fell below 20,000 and even below 15,000 USD. Other categories had averages above 80,000 USD. It would be wise to conduct a few more tests before we decided whether to include the goal variable within our models. 

Lastly, I wanted to make one final comment before proceeding to the next section. Although we observed several key variables in this introductory section, in the next section we'll observe a few more variables to determine if there is any sort of relation between those variables and our two dependent variables of interest "USD Pledged Real" and "Status". From there we'll not only select our predictor variables but select which dependent variable we'll be creating a model for. 

### **<a href="#top">Back to top</a>**


## Variable Selection

### Plotting against the Dependent Variables

**Goals**

When selecting variables, I'll be making any variable selections based on the plots in this section or from previous sections. The goal here is to try and compile sets of predictor variables for each of our two dependent variables. The first dependent variable is the status of the project. The second dependent variable is for the amount pledged in real USD. From there, we'll decide which predictive model we'll pursue based on which has a more complete and appropriate set of variables

**Considerations**

One important consideration is that I must ensure that any variables selected are included into the model with good reason. My goal here is to get a model that gives the most accurate predictions it can (given the risk and uncertainty involved in kickstarter projects). With that being said, while getting accurate predictions in this scenario may require a more complex model I should do my best not to overfit the model to this sample. Therefore for each variable or collection of variables I select, I will briefly discuss why each variable is applicable and appropriate.

There may be some additional set up needed for each variable at this point. To start, I'll need to convert the dependent variable status to a binary (0 and 1). In this case, it makes more logical sense to associate 0 for failed projects and 1 for successful projects. When it comes to the set up for other variables, I'll discuss this in their relevant subsections.

Below is the code used to convert the status variable to a binary. 

```{r Kickstarter2}
Kickstarter$status_bin<- ifelse(Kickstarter$status=="Successful", 1, 0)
```

### Project Status

To start, we will be selecting our variables for one of our potential dependent variables, *Status*. 

##### A reminder on the Findings from Exploratory Section

Lets briefly recall some plots created in the previous section. Specifically, I wanted to  examine the three plots we created to visualize the failed vs successful projects sorted by category.

```{r PrevPlots, echo = FALSE}
Plot
Plot2
Plot3
rm( "Plot", "Plot2", "Plot3")
```

**Why are such variables appropriate for this model?**

As is apparent in the plot, some categories had higher success rates than failures, some categories also had extremely low success rates. This lends evidence that there may be some sort of association between category and if a project is successful or not. For that reason, I'll be incorporating all of the categories as dummy variables within the predictive models created for status. Assuming we decide to produce a model for our dependent variable *status*, we'll likely see that the coefficient estimates will reflect how categories may influence the probability of a project's success.

##### Duration

Before creating any sort of plot to see the distribution of project duration against the project status I need to create a new variable called *duration*. To do this I'll be using two existing variables *deadline* and *launched*, and take the time difference across those two columns. I chose to do this in excel as it allowed me to quickly determine if its better to measure the duration of the project in days or months. After playing around with the data, it seems the best way to measure duration was in days. I implemented this into the original dataset that was imported.

Once created, I determined it would be interesting to see if there was any noticeable pattern with duration and status. Also to improve readability I wanted to create a random sample of about 1000 values to plot it. I figured it would be hard to make anything out with 300,000 values listed on a scatterplot. Additionally, I decided to limit the y axis to 100 days in duration since it seemed a vast majority of projects did fall within this time duration. This prevented the outliers from making the plot more difficult to read.

```{r Duration, warning= FALSE, echo = FALSE}
set.seed(101)
Set_Sample <- sample(1:372300, 1000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample,]


ggplot(KickstarterScatter, aes(x= status_bin, y= duration, col = status ))+
  geom_point()+
  geom_jitter()+
  scale_y_continuous(limits=c(0,100))+
  labs(x="Status", y="Project Duration (Days)",
       title="Status vs Project Duration")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 

rm("Duration")
```

Interestingly enough, it seems that project duration doesn't have a clear relationship to the status of a project. As is apparent in the plot, both failed and successful seem to overlap  consistently as duration increases. In fact, these plots are actually almost identical in terms of distribution. Longer duration does not necessarily mean a higher instance of success, as I hypothesized. Additionally, shorter duration does not ensure a higher rate of success either. For this reason, I'll avoid including duration in my predictive models for project status. 


##### Country 

I wanted to see if the variable *country* had any relationship to the success or failure of a project. To do this, I wanted to approach it in a manner similar to how I plotted categories. This meant breaking it up into three groups as I did before and plotting each group via a bar graph. 

```{r CountrySuccess, echo = FALSE}
Group1a <- filter(Kickstarter, country == "SE")
Group1b <- filter(Kickstarter, country == "IE")
Group1c <- filter(Kickstarter, country == "IT")
Group1d <- filter(Kickstarter, country == "MX")
Group1e <- filter(Kickstarter, country == "NZ")
Group1f <- filter(Kickstarter, country == "ES")
Group1g <- filter(Kickstarter, country == "DK")
Group1h <- filter(Kickstarter, country == "US")


Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)
Group1 <- rbind(Group1, Group1f)
Group1 <- rbind(Group1, Group1g)
Group1 <- rbind(Group1, Group1h)


rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e", "Group1f", "Group1g", "Group1h")

ggplot(Group1)+ 
  geom_bar(aes(country, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Country (Group 1)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("Group1")

Group1a <- filter(Kickstarter, country == "AT")
Group1b <- filter(Kickstarter, country == "CA")
Group1c <- filter(Kickstarter, country == "GB")
Group1d <- filter(Kickstarter, country == "NO")
Group1e <- filter(Kickstarter, country == "NL")
Group1f <- filter(Kickstarter, country == "AU")
Group1g <- filter(Kickstarter, country == "DE")
Group1h <- filter(Kickstarter, country == "BE")


Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)
Group1 <- rbind(Group1, Group1f)
Group1 <- rbind(Group1, Group1g)
Group1 <- rbind(Group1, Group1h)

rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e", "Group1f", "Group1g", "Group1h")

ggplot(Group1)+ 
  geom_bar(aes(country, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Country (Group 2)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("Group1")

Group1a <- filter(Kickstarter, country == "CH")
Group1b <- filter(Kickstarter, country == "FR")
Group1c <- filter(Kickstarter, country == "SG")
Group1d <- filter(Kickstarter, country == "LU")
Group1e <- filter(Kickstarter, country == "JP")



Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)

rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e")

ggplot(Group1)+ 
  geom_bar(aes(country, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Category", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Country (Group 3)")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("Group1")
```

**Considerations** 

When plotting these three graphs I wanted to see if any country really stuck out in terms of its success rate or failure rate. Because I also had a variable for currency, I decided that I would either select currency, country, or neither. The reason for this is because the variables are very similar in nature. For that reason, I won't be deciding in this subsection whether or not I'll be selecting country, but will be making a few comments instead. The decision between selecting country, currency, or neither variable will instead be made after the currency subsection. 

**Comments**

Although we won't be deciding on whether to select this variable in this subsection, I wanted to briefly comment on any features of interest across all three plots. Although it was only a small deviation, I did notice that Austria (AT) from plot Group 2 had a success rate that fell below .2 within the sample. A second deviation was observed for Italy (IT) which also fell below the success rate of .2 within the sample. I thought I should point this out, however even though these are points of interest that could indicate some relationship I won't make any comments on relationship within this subsection. Any determination on whether we will select country, currency, or neither will be discussed after the currency section.

##### Goal in Real USD Revisited

When we plotted the distribution of project goals in real USD earlier, we first plotted the distribution of goals across all categories up to the $200,000 range. With this visual I'll be seeking to plot the goal in real USD against the status of the project. Once again I'll be using the random sample I generated earlier to improve the readability of the plot.

```{r GoalScatter, warning= FALSE, echo = FALSE}
set.seed(101)
Set_Sample <- sample(1:372300, 1000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample,]


ggplot(KickstarterScatter, aes(x= status_bin, y= usd_goal_real, col = status ))+
  geom_point()+
  geom_jitter()+
  scale_y_continuous(limits=c(0,200000))+
  labs(x="Status", y="Goal (In Real USD)",
       title="Status vs Goal in Real USD (Goals < $200,000)")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 
```

While it's not the strongest of relationships, we can see that once a project passes the 50,000 USD mark there is a higher instance of failure. Putting this in broader terms, even though its not an obvious relationship, we can see that a relationship does exist where higher goal projects begin to experience a higher instance of failure. For that reason, it would be a good idea to include this variable in our model for predicting project status. We'll likely see that for every dollar increase in our goal there is a decrease in the probability of success. We'll know for sure if we decide to create a predictive model for *status*.

##### Backers

Another variable I wanted to look at was backers. Specifically, within this sample I wanted to know if it was at all apparent that more backers lead to higher instances of successful projects? To find out, I decided to create a scatter plot of the status of the project against the number of backers. Like with the previous plots, I'll be using the random sample I generated earlier (now with 2,000 observations) to improve readability. Note there were a few projects that actually gained support from over 5,000 backers (a few with over 20,000!). If this has not already been made clear, I should make it clear that those projects were successful. Lastly, to further improve readability, I also limited the y-axis (number of backers) to just 5,000.

```{r Backers, warning= FALSE, echo = FALSE}
set.seed(101)
Set_Sample2 <- sample(1:372300, 2000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample2,]


ggplot(KickstarterScatter, aes(x= status_bin, y= backers, col = status ))+
  geom_point()+
  geom_jitter()+
   scale_y_continuous(limits=c(0,5000))+
  labs(x="Status", y="Number of Backers",
       title="Status vs Number of Backers (Backers < 5000)")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 
```

**Findings**

As is shown in the plot, As the number of backers increase there are higher instances of successful projects. This indicates there does exist some sort of relationship between the success of a project and the number of backers. Just from this alone, it was clear that it would be wise to include this variable in our model for predicting probability of success or failure. Even though I've already decided to include this model, out of curiosity, I also wanted to take a closer look at projects with backers numbering below 1,000 and 100.

```{r Backers2, echo= FALSE, warning= FALSE}
set.seed(101)
Set_Sample2 <- sample(1:372300, 2000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample2,]


ggplot(KickstarterScatter, aes(x= status_bin, y= backers, col = status ))+
  geom_point()+
  geom_jitter()+
   scale_y_continuous(limits=c(0,1000))+
  labs(x="Status", y="Number of Backers",
       title="Status vs Number of Backers (Backers < 1000)")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 

set.seed(101)
Set_Sample2 <- sample(1:372300, 2000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample2,]


ggplot(KickstarterScatter, aes(x= status_bin, y= backers, col = status ))+
  geom_point()+
  geom_jitter()+
   scale_y_continuous(limits=c(0,100))+
  labs(x="Status", y="Number of Backers",
       title="Status vs Number of Backers (Backers < 100)")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 
```

**Additional Comments**

By observing the additional plots we can see that within the sample there is a much higher instance of failure as the number of backers falls below 25. It should also be noted that even if a project has over 250 backers, our data indicates that these projects aren't necessarily "out of woods" and can still fail. 

These additional plots reinforce my conclusion that it would be wise to include the variable for Backers in my predictive model. Once again, we'll know for sure if we decide to create a predictive model for status, but these findings seem to indicate that as the number of backers increase the probability of success increases.

##### Amount Pledged in Real USD

After some deliberation, I decided that it would be wise not to include amount pledged in this model for the following reason. When running a predictive model for project status, it would be pointless to include amount pledged. This is because if the amount pledged is greater than our goal it's obvious the kickstarter would be successful in a financial sense. We're assuming that the model is being used to predict the probability of success for a project that is still in progress. In this case, amount pledged isn't variable that should be included in our model.

#### Project Currency

Like I did with categories and country, I wanted to see if the currency collected for the project had any bearing on success or failure. Maybe certain currencies have some relationship with a higher instance of successful projects.

```{r CurrencySuccess, echo = FALSE}
Group1a <- filter(Kickstarter, currency == "USD")
Group1b <- filter(Kickstarter, currency == "CAD")
Group1c <- filter(Kickstarter, currency == "GBP")
Group1d <- filter(Kickstarter, currency == "NOK")
Group1e <- filter(Kickstarter, currency == "EUR")
Group1f <- filter(Kickstarter, currency == "AUD")
Group1g <- filter(Kickstarter, currency == "SEK")

Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)
Group1 <- rbind(Group1, Group1f)
Group1 <- rbind(Group1, Group1g)

rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e", "Group1f", "Group1g")

ggplot(Group1)+ 
  geom_bar(aes(currency, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Currency", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Currency")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("Group1")

Group1a <- filter(Kickstarter, currency == "HKD")
Group1b <- filter(Kickstarter, currency == "MXN")
Group1c <- filter(Kickstarter, currency == "DKK")
Group1d <- filter(Kickstarter, currency == "NZD")
Group1e <- filter(Kickstarter, currency == "JPY")
Group1f <- filter(Kickstarter, currency == "CHF")
Group1g <- filter(Kickstarter, currency == "SGD")

Group1 <- rbind(Group1a, Group1b)
Group1 <- rbind(Group1, Group1c)
Group1 <- rbind(Group1, Group1d)
Group1 <- rbind(Group1, Group1e)
Group1 <- rbind(Group1, Group1f)
Group1 <- rbind(Group1, Group1g)

rm("Group1a", "Group1b", "Group1c", "Group1d", "Group1e", "Group1f", "Group1g")

ggplot(Group1)+ 
  geom_bar(aes(currency, fill= status, y=..count../tapply(..count.., ..x.. ,sum)[..x..]), position="dodge")+
  scale_y_continuous(limits=c(0,1), breaks = seq(0,1,.2))+
  scale_fill_manual(name = "Status", labels = c("Failed", "Successful"), values=c("sienna2", "deepskyblue4"))+
    labs(x="Project Currency", y="Proportion of Total",
       title="Kickstarter: Failed vs Successful Projects by Currency")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("Group1")
```

**Comments on Currency**

While there was some variation, it didn't seem like there was any notable difference in success and failure rates across currency. All the currencies listed seemed to fall between 20% and 40% of the projects within the sample succeeding. There was no specific currency here that really stuck out. Because of this, I decided that I would not be selecting currency, as I didn't want to risk any potential overfitting of the data. Despite the fact I've made this determination within this section, we'll discuss any considerations made to determine whether we select currency, country, or neither predictor variable. 

##### Variable Selection: Currency, Country, or Neither

**Why Choose One or the Other?** 

To start, I wanted to only include one of the two variables or neither. It's worth mentioning that these two variables come to represent very similar things. For example, projects based in Great Britain operated solely in GBP (Pound Sterling). At this point you may be wondering, why even test between two of these variables, wouldn't both variables produce the *same results*? While we may be inclined to reach such conclusions, there is one key difference however, and that is that some countries may share the same currency. Take for example the continent of Europe, multiple countries within Europe use the Euro, and more specifically within our Kickstarter data there are several countries that use Euros. Essentially we're receiving an average of all countries that use the Euro as currency. In this sense, while extremely similar there are some difference between the two variables.

**Decision**

I ultimately decided to select neither variable. My reasoning is that even though there are small differences between success rates in each country we can not conclusively determine if this is really due to the country itself or because of some other factor. For the sake of example, let's look at Italy. The process used to isolate the country of italy for analysis is listed below:

```{r BarplotItaly}

ItalyData <- filter(Kickstarter, country == "IT")

ggplot(ItalyData, aes(main_category), position="dodge")+ 
  geom_bar()+
    labs(x="Project Category", y="Total Count of Projects",
       title="Kickstarter: Category Count for Italy")+ #Label
   theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())

rm("ItalyData") 
#Removed data to speed up R processing for future chunks
```

**Analysis**

Looking at the categories with the highest counts, we see that the highest was technology, followed by games, design, fashion, and then film and video. Note that category  technology had a high rate of failure within our entire sample, design had a lower rate of success, fashion had a low rate of success, and film and video had just an average rate of success. Also note that there weren't many *if any* projects belonging to categories where the success rate was higher than the failure rate. 

Given the plot and accompanying description it could very well be that country's may be *under-performing* due to the distribution of categories. For that reason, it becomes harder to pinpoint how much the country of origin for a project influences a project's chances of succeeding. For this reason, I've decided not to include this variable in my predictive model. 
##### Final Selection of Variables for Project Status

Thus far for our predictive model for status it was decided that the following variables would be used: 

* Project Goal in Real USD

* Project Categories (As dummy variables)

* Number of Backers

Now that I have a collection of variables to use,  I'll create a new data.frame with just these variables. I do this because I am striving to keep my data organized throughout this research process. Should I have a sufficient amount of predictor variables to work with for our dependent variable for amount pledged, I'll do the same.

```{r ModelDF}
StatusDF <- data.frame(Kickstarter$status_bin, Kickstarter$main_category, Kickstarter$usd_goal_real, Kickstarter$backers)

names(StatusDF) = c("status", "main_category", "usd_goal_real", "backers")
```

**Fail safes in Place to Prevent Over-Fitting and Bias**

I've only sought to include variables were relationships were evident. By only selecting necessary variables where it seemed applicable to observations outside the sample data set, I worked to reduce over-fitting. I've also tried to avoid bias by not selecting predictor variables that may have considerable influence over one another. Take for example country, as we said earlier it could be that the category distributions for each country may influence the success rates and failure rates for each country. The predictor variables selected were intended to have minimal if any influence on one another. 

I'll be performing forward step-wise selection along with the creation of my first model to ensure that the variables selected only positively impact model accuracy. Additionally, I'll ensure that only variables significant at a 5% level are included. 

### Amount Pledged in Real USD

A second predictive model I wanted to create was a predictive model that could potentially predict the amount that would be pledged in Real USD to a project based on a few variables. Once again, this would likely have a lot of variance, as with startup projects there is a high degree of uncertainty. 

For now, we have a solid set of variables to include when creating a predictive model for project status. I'll test out a few key variables to see if given the current dataset this would be a model worth pursuing. Whatever the decision may be, I at least wanted to show my reasoning for why I may or may not choose to pursue this predictive model.

The variables I'll be plotting against amount pledged will be the goal in real USD and the number of backers. Based on the results for these two variables, I'll decide if its worthwhile to plot all the main categories against amount pledged. I'll plot these two predictor variables against amount pledged in real USD and determine if the dataset in its current state is capable of predicting the amount pledged to a kickstarter project. It is entirely possible that this dataset will not work as intended since it could arguably require even more specific variables than our predictive models for status. 

Because Kickstarter is quite a unique platform, finding additional variables to make this model work (should it not) will be extremely time-consuming and still may not provide sought-after results. If it seems that a model for amount pledged won't generate as meaningful results as the model for status, I'll opt to focus more on the predictive model for project status. We'll start our assessment of how feasible this model is by plotting the Goal in Real USD.

##### Goal in Real USD Against Amount Pledged

```{r GoalScatter2, echo = FALSE, warning=FALSE}
set.seed(101)
Set_Sample2 <- sample(1:372300, 1000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample2,]

options(scipen = 999)

ggplot(KickstarterScatter, aes(x= usd_goal_real, y = usd_pleged_real, col = status))+
  geom_point()+
   scale_x_continuous(limits=c(0, 20000))+
  scale_y_continuous(limits=c(0, 20000))+
  labs(x="Goal in Real USD", y="Amount Pledged in Real USD",
       title="Goal vs Amount Pleged")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 
```

**Analysis**

If you observe this relationship there is something quite peculiar that can be noticed. For amounts pledged below 10,000 and goals set below 10,000, it appears as if there is a right triangle that could be fit into that space. I decided to color points by the status to better explain why this occurs. It seems that most of successful projects have pledges amounting to the goal or just slightly above the goal. Many failed projects tend to have pledges which total closer to 0 (with a few exceptions). Essentially due the distribution of the data in the plot, it would seem that the goal for the projects in real USD doesn't have a strong relationship with the amount that's pledged. 

At this point it seems that both goal in USD and the number of backers are quite intuitive and expected. It wouldn't make much sense to create a model with these variables. Despite the fact that it doesn't seem we'll end up creating a predictive model for amount pledged, I'll create a plot for backers against the amount pledged anyways.


#### Backers

```{r Pledge2, warning= FALSE, echo = FALSE}
set.seed(101)
Set_Sample2 <- sample(1:372300, 1000, replace=F)
KickstarterScatter <- Kickstarter[Set_Sample2,]

options(scipen = 999)

ggplot(KickstarterScatter, aes(x= backers, y = usd_pleged_real))+
  geom_point()+
  geom_smooth(se = FALSE)+
   scale_x_continuous(limits=c(0, 750))+
  scale_y_continuous(limits=c(0, 50000))+
  labs(x="Backers", y="Amount Pledged in Real USD",
       title="Backers vs Amount Plegded")+ #Label
  theme(plot.title=element_text(color="dimgray", size = 15, face="bold"))+ #Title customizer
  theme(axis.line.y=element_line(color="light grey"))+ #Axis lines customizer
  theme(axis.ticks =element_line(color="light grey"))+
    theme(strip.background = element_blank(), strip.text.x = element_text(color = "dimgray", size = 12, face = "bold"))+
  theme(panel.background = element_blank())+
  theme(legend.key=element_blank()) 

```

As is expected as the number of backers increases so too does the amount pledged. Despite this, We see the there is a fanning effect which could also be problematic for creating a model. Additionally, it does seem that the data set we have as it stands is more suited for predicting success or failure rather than an amount that would be pledged. ***At this point it seems that a better model to create is a predictive model for status.**

This doesn't effect the goals of my research too much, as these two models do indicate similar things. However, it would seem that creating a predictive model for status may be more worthwhile and feasible with the current data set.   

### Final Decision

**For this research, my findings indicate that it is more worthwhile to pursue and focus on creating a predictive model for kickstarter project statuses. After much thought these two potential models predict similar things, however as a startup or even investor you'd be more concerned with your chances of success or failure in hitting your goal (the project status must be successful to even receive funding), rather than hitting a specific dollar amount. For this reason I'll be pursuing a predictive model for project status rather than amount pledged in real USD.**

### **<a href="#top">Back to top</a>**



## Validation 

### Validation

In the previous section, it was determined that we would be creating a collection of predictive models that could predict the probability that a kickstarter project would succeed or fail. The predictor variables selected included the main categories (as dummy variables), the goal set for the project in real USD, and lastly the number of backers. 

in this section we'll be creating some validation sets. Prior to creating the training and test set however, we'll first convert the 15 categories into dummy variables. For the purpose of demonstration I'll convert all the variables to dummy variables, however, I'll be leaving out the category "games" and I will use it as a reference. 

#### Converting the Main Categories to Dummy Variables

As we just discussed, we'll convert the main categories to dummy variables here. The conversion is as follows: 

```{r DummyVar_Creation}
# To improve processing speeds in R I'll remove this large dataset since it's no longer relevant.

rm("Data_2018")

StatusDF$film <- ifelse(StatusDF$main_category=="Film & Video",1,0) 
StatusDF$design <- ifelse(StatusDF$main_category=="Design",1,0) 
StatusDF$tech <- ifelse(StatusDF$main_category=="Technology",1,0) 
StatusDF$games <- ifelse(StatusDF$main_category=="Games",1,0) 
StatusDF$food <- ifelse(StatusDF$main_category=="Food",1,0) 
StatusDF$music <- ifelse(StatusDF$main_category=="Music",1,0) 
StatusDF$photo <- ifelse(StatusDF$main_category=="Photography",1,0) 
StatusDF$publish <- ifelse(StatusDF$main_category=="Publishing",1,0) 
StatusDF$art <- ifelse(StatusDF$main_category=="Art",1,0) 
StatusDF$fashion <- ifelse(StatusDF$main_category=="Fashion",1,0) 
StatusDF$theater <- ifelse(StatusDF$main_category=="Theater",1,0) 
StatusDF$crafts <- ifelse(StatusDF$main_category=="Crafts",1,0) 
StatusDF$journalism <- ifelse(StatusDF$main_category=="Journalism",1,0) 
StatusDF$comics <- ifelse(StatusDF$main_category=="Comics",1,0) 
StatusDF$dance <- ifelse(StatusDF$main_category=="Dance",1,0) 

#Remember we'll be using the category games as a reference so we'll be using 14 of these dummy variables. 
```

##### Creating a Training Set and Validation (Test) Set

For this next subsection I'll be creating a training set and validation set. I decided I would use a **normal validation set approach**, where I randomly select a training set and validation set. When creating the sets I'll be assigning around 85% of the data to the training set and around 15% to the test set. In addition, I'll randomly assign each observation to either set. 

```{r ValidationSets}
set.seed(102)
status_sample <- sample(1:372300, 316455, replace=F)
StatusTrain <- StatusDF[status_sample,]
StatusTest <- StatusDF[-status_sample,]

rm("Kickstarter", "KickstarterScatter")
```

Now that the test set and training set have been created randomly, I can now create my models. To start I'll create a few generic models that we've grown accustom to for binary dependent variables. These include Linear Probability models (LRM), logistic regression (glm), quadratic Discriminant Analysis, and KNN. We'll conduct cross validation and construct confusion matrices to assess accuracy, and also examine specificity and sensitivity. We'll also create an alternate model via a neural network as a final model. 

When creating the models we'll assess how feasible they are by observing the accuracy, from there we'll decide whether or not the model is worth further analysis, or if it's in our best interest to move onto another potentially more accurate model.

###### A Final Test for Variable Selection

As discussed in previous sections when we proceed to the model creation section, we'll run a forward step-wise selection to assess if any variables can be removed without penalizing accuracy. This will ensure that we have in fact implemented all relevant variables into our model. We'll run this forward step-wise selection in the first model we create, and assess how removing variables impacts our model's accuracy.

#### A Roadmap From this Point

To highlight the rest of this research as it stands, once we have assessed accuracy, specificity, and sensitivity of each model we'll discuss why a certain model may be preferable over another. We'll end this series of sections by selecting the most suitable model of all models that were created. Lastly, I'll discuss some considerations for future research, and any prospects for expanding the data set. I'll briefly wrap up by discussing what the findings of this research were, some of the potential applications, and room for improvement in the future (for both the dataset and final models).

### **<a href="#top">Back to top</a>**


## Creating Models

### Logistic Regression Model

We'll now create logistic regression model and run a confusion matrix. Once again, note that I'll be starting out by using the *games* category as a reference. 

```{r LRM1, echo = FALSE} 
LRM <- glm(status ~ usd_goal_real + backers + film + design + tech + food + music + photo + publish + art + fashion + theater + crafts + journalism + comics + dance, family = binomial, data = StatusTrain) # This is logistic regression on all 16 variables with games as a reference.
summary(LRM)

Pred3 <- predict(LRM, StatusTest, type = "response")
Binary3 <- round(Pred3); #Using round sets cutoff point to .5

100*mean(StatusTest$status == Binary3) #Basically gives us an overall accuracy
```

The accuracy is much higher than anticipated! With an accuracy of 89.75%, our error is only 10.25%. This all looks promising, but I wanted to see if we could improve accuracy at all by removing variables from the model. I used forward step-wise selection in the next subsection to remove variables one by one and observe if accuracy was at all improved. To ensure I'm maximizing accuracy, I included the variable *games* once again.

##### Running a Forward Step-wise Selection To Confirm Variable Selection

Before proceeding onto future models, I wanted to confirm that 16 variables was in fact the optimum amount of variables to include the model. I used forward step-wise selection, and then implemented my findings in a sample model, to observe if accuracy dropped following variable removal. 

```{r ForwardStepwise, echo = FALSE}
regfit.fwd <- regsubsets(status ~ usd_goal_real + backers + film + design + tech + food + music + photo + publish + art + fashion + theater + crafts + journalism + comics + dance, data = StatusTrain ,nvmax= 14, method ="forward")
summary(regfit.fwd)
```

It was found that my previous intuition was right, and leaving games out of the model, was a good decision. I further lowered the number  of variables to 15 from here to see if any additional variable should be left out. According to the forward step-wise selection, the next best variable to remove was design. However, after removing the design variable my accuracy saw a small drop. To ensure this trend would continue I removed the next *weakest* term, the variable *photo*. Once again the accuracy took a small dip following the removal of an additional variable. This indicated that 16 variables was in fact the optimum subset size. To demonstrate this, observe how the model accuracy drops in model below after removing design and photo. 

```{r Model4Update, warning = FALSE, error = FALSE}
Model4 <- glm(status ~ usd_goal_real + backers + film + tech + food + music + publish + art + fashion + theater + crafts + journalism + comics + dance, family = binomial, data = StatusTrain) #Model 3 is logistic regression on all 3 variables.

Pred4 <- predict(Model4, StatusTest, type = "response")
Binary4 <- round(Pred4); #Using round sets cutoff point to .5

100*mean(StatusTest$status == Binary4) #Basically gives us an overall accuracy

rm("Binary4", "Pred4", "status_sample", "Model4")
```

#### Returning to the Logistic Regression Model

Knowing I had the optimum number of variables in my model, I wanted to test it out, I created a simulation where the goal was 10,000 USD and the backers ranged from 0 to 200, for a technology project. The resulting plot is below:

```{r glmplot, echo = FALSE}
Logit_Sim <- predict(LRM, data.frame(usd_goal_real = 10000, backers = 0:200, design = 0, film = 0, tech = 1, food = 0, music = 0, photo = 0, publish = 0, art = 0, fashion = 0, theater = 0, crafts = 0, journalism = 0, comics = 0, dance = 0), type = "response")

plot(0:200, Logit_Sim, type = "l", lwd = 1, col = "red", xlab = "Backers", ylab = "Probability", main = "Probability of Success, Tech Project with $10,000 Goal")

```

**A Brief Discussion About the Plot**

As you can see for the most part, as backers increase the probability of success gets closer and closer to one. In this scenario success is almost guaranteed at around 150 backers. 

With this model we can set a range for the goals, or set a range for the backers. We are also able to set the category of the project as this can influence the probability of success. Personally, how I would use this model is by going to the kickstarter website, selecting a random project (documenting category and goal), and then I'd observe how many backers the project would need to almost guarantee success (probability > 90%). 

**In fact, because this model is so intuitive I will recreate a similar prediction plot in the application section of this report.** However, before we do this, we'll select the best model out of all the models we create. 

**Analysis of the Model**

```{r SumModel3, echo = FALSE}
summary(LRM)
100*mean(StatusTest$status == Binary3) 
```

This model was much more accurate than I had anticipated! It should also be noted that the glm.fit will contain some fitted probabilities that are virtually equal to 0 or 1. Within the context of the dataset such findings shouldn't worry us too much. Probabilities of success equaling close to 0 or 1 make sense for the following reasons in context of the data:

First and foremost, looking at our predictor variable for backers, having 0 backers would put the probability of success at virtually 0. Additionally, with some projects in our dataset having over 20,000 backers some projects have a probability of success very close to 1. In addition to our predictor variable for *backers*, our predictor variable for *goals in real USD* could help to explain why some fitted probabilities are very close to 0 or 1. Some projects on kickstarter had goals above the $10,000,000, a goal that is highly improbable of being reached via kickstarter. 

Moving onto the coefficient estimates, it was evident that these estimates actually correlated with our findings during our exploratory section. Projects which tend to have higher rates of failures or higher rates of success are reflected in the coefficient estimates. Take for example theater or dance, which had coefficient estimates of 2.366998135 and 2.388282166 respectively. This shows that having projects in these categories can increase your probability of success more than tech which had a coefficient estimate of 0.880131301. Additionally, we see via the coefficient estimates that as the number of backers increases so too does the probability of success. Lastly, for every dollar the goal increases the probability of success decreases by -0.000195364. We see this reflected especially when a project has a goal of 1,000,000 (some projects did in fact have goals this high). It becomes increasingly improbable for these projects to succeed. 

### Linear Probability Model

In our first model we sought to create a logistic regression model. Now, I'll create a linear probability model and assess if it is any more accurate than the previously created model. 

**NOTE: When initially creating the model, is was found that the variable for design was no longer significant. In an effort to only maintain a model with variables that are within the 5% level of significance I removed this variable from the model.** 

```{r LinearProbModel, warning = FALSE, echo = FALSE}
LPM <- lm(status ~ usd_goal_real + backers + film + tech + food + music + photo + publish + art + fashion + theater + crafts + journalism + comics + dance, data = StatusTrain) # This is logistic regression on all 16 variables with games as a reference.
summary(LPM)

PredLin <- predict(LPM) #Gives predictions between 0 and 1
BinaryLin <- round(PredLin) #Anything below .5 rounds to 0 anything above round to 1
100 * mean(StatusTest$status== BinaryLin)

rm("PredLin", "BinaryLin", "LPM")
```

**Findings**

It appears that linear models lead to a much lower level of accuracy. As you can see, the accuracy here is only 64.88% or .6488. This leads to an error of 35.12%, which compared to the LRM model is far from preferable.  Because the accuracy is much lower than our logistic regression model, I've decided not to include this model in our final model consideration. Even if the sensitivity and specificity end up higher than the LRM (it doesn't appear likely), this model is not accurate enough compared to other models. Because this model doesn't seem feasible, I'll leave all analysis here and move onto our next potential model *QDA* or *Quadratic Discriminant Analysis*.

We'll follow the same process when running our QDA model. We'll take a look at the accuracy of the model, and determine if its worth the additional analysis, or if we should move onto our final model *KNN*, before proceeding to a less-common alternative model, neural networks. 


### QDA: Quadratic Discriminant Analysis

Now I'll create a QDA model and assess if its accuracy and some of the basic model features. 
Additionally, I'll determine if we should include the model in a "*final running*" or if we should proceed onto our next model KNN (K Nearest-Neighbor).

```{r QDAModel, echo = FALSE}
qda.fit <- qda(status ~ usd_goal_real + backers + film + tech + food + music + photo + publish + art + fashion + theater + crafts + journalism + comics + dance, data = StatusTrain)

qda.fit
qda.pred <- predict(qda.fit, StatusTest)
confusion(StatusTest$status, qda.pred$class)

rm("qda.fit", "regfit.fwd", "qda.pred", "Set_Sample", "Set_Sample2")
```

After running a QDA model, it would appear the accuracy was even worse than with our linear probability model. The accuracy was found to be only 61.2% compared to the linear probability model (64%). This leaves us with an error of about 38.8%. Since this model is even more inaccurate than the LPM model, I've decided this will not be one of the final models we'll be considering. Additionally, because this model is more inaccurate, I'll leave the analysis of this model here, so we can move on to a potentially feasible model "KNN".

### KNN: K Nearest-Neighbor

Before creating the neural network model we will create one more model and assess if it should be among the final models we consider. Before performing the KNN method, I'll first need to modify the datasets. I'll do this by creating a new dataframe that only includes the predictor variables we're interested in. I'll also create a new value *knn_status* which essentially includes the statuses of our training set (0 or 1).

```{r KNNData}
KNNTrain <- data.frame(StatusTrain$usd_goal_real, StatusTrain$backers, StatusTrain$film, StatusTrain$design, StatusTrain$tech, StatusTrain$food, StatusTrain$music, StatusTrain$photo, StatusTrain$publish, StatusTrain$art, StatusTrain$fashion, StatusTrain$theater, StatusTrain$crafts, StatusTrain$journalism, StatusTrain$comics, StatusTrain$dance)

KNNTest <- data.frame(StatusTest$usd_goal_real, StatusTest$backers, StatusTest$film, StatusTest$design, StatusTest$tech, StatusTest$food, StatusTest$music, StatusTest$photo, StatusTest$publish, StatusTest$art, StatusTest$fashion, StatusTest$theater, StatusTest$crafts, StatusTest$journalism, StatusTest$comics, StatusTest$dance)

knn_status <- StatusTrain$status

rm("Logit_Sim")
```

Now that we've create the relevant data frames, what  I'll do here is begin testing k-values. I'll continue to increase the k-value until I see accuracy consistently decreasing or leveling off. At that point I'll stop increasing the k-value and assess whether or not the KNN model would be a feasible final model to choose from. I'll briefly comment on the accuracy as we proceed through k-values. 

**NOTE: For the purposes of minimizing the knit time, I'll summarize my findings for each k-value and just include the confusion matrix for my selected k-value. Because we're using such a large training set with greater than 300,000 observations, KNN operations can take a few minutes. We'll continue this discussion within our conclusion, but KNN algorithms tend to slow down as the volume of data increases.**

**Accuracy at K = 1** 

Overall accuracy = 0.906 

To start, the accuracy of this model at k=1 is already slightly higher than the logistic regression model we created with an accuray of .906. This leaves an error of only 9.4%. I'll continue to increase the K-value from here.

**Accuracy at K = 2**

Overall accuracy = 0.908 

After raising the K value to 2 the accuracy slightly increased to .908. I'll further increase the k-value to k = 3.

**Accuracy at K = 3**

Overall accuracy = 0.917 

After increasing the K value to 3 the accuracy increased by .09 to .917. I'll continue to increase the k-value to k = 4.

**Accuracy at K = 4**

Overall accuracy = 0.919 

When increasing the K value to 4 the accuracy increased to .919. Because there has been another increase, I'll increase the k-value to k=5.

**Accuracy at K = 5**

Overall accuracy = 0.921 

When increasing the K value to 5 the accuracy increased to .921. Because there has been another increase, I'll increase the k-value to k=6.

**Accuracy at K = 6**

Overall accuracy = 0.921 

After increasing the K value to k = 6, the accuracy remained at .921. I'll increase the k-value once more to determine if the k-value will begin decreasing from this point. Should it decrease again I'll select k=6 as the optimum k value. The reason being that as you increase the flexibility of the model (lower of number neighbors or lower K-value) the risk of overfitting does seem to increase. With so many variables, I want to minimize overfitting, so I'll select the higher K-value.

**Accuracy at K = 7**

Overall accuracy = 0.923 

After increasing the K value to K = 7 the overall accuracy increased once more to .923. Once again I'll increase the k-value to k=8.

**Accuracy at K = 8**

Overall accuracy = 0.922 

After increasing the k value to k = 8 the overall accuracy decreased to .922. I'll increase the k-value to k = 9 to see if there is another decrease. If the accuracy decreases again we would say that the optimum k-value would be k = 7.

**Accuracy at K = 9**

Overall accuracy = 0.923 

After increasing the k-value to k = 9, the overall accuracy to once again increased to .923. Because we want to reduce risk of overfitting, I'll be opting to use k = 9 over k = 7. I'll continue to increase the k-value until it decreases consistently. 

**Accuracy at K = 10**

Overall accuracy = 0.923

After increasing the K-value to k = 10, the overall accuracy remained at .923. To reduce risk overfitting I'll use k=10 over k = 9. We'll continue to increase to k = 11. 

**NOTE: If the accuracy remains at .923 at k = 11 for the third time in the row, we'll use k = 11 as our optimum value.**

**Accuracy at K = 11**

```{r KNN11}
set.seed(101)
knn.pred <- knn(KNNTrain, KNNTest, knn_status, k=11)
confusion(StatusTest$status, knn.pred)
```

Once again, after increasing the k-value the overall accuracy remained at .923. Since the accuracy has remained at .923 consistently, our time would be better served moving on to the subsection where we determine which models we'll consider against the neural networks. In terms of accuracy, the logistic regression and KNN models scored the highest. We'll be comparing performance of these models against each other and potentially against the neural network. 

##### The Optimum K-Value

Because the accuracy seemingly peaked at .923 and remained at .923 from k = 9 to k = 11, I'll be selecting k = 11 as the optimum k-value. As we discussed earlier, the higher the k-value the more we reduce the risk of overfitting. 

### Models in Contention

Of the models we looked at we decided the linear probability model and quadratic discriminant analysis models were too inaccurate compared to our other two models. As a result, we'll be comparing the performance of our logistic regression model, KNN model, and potentially our neural network model. After comparing the performance, we'll select a final model and proceed to the conclusion where we'll discuss a sample application, plot a sample kickstarter project using the logistic regression model, sum up our findings, discuss further applications of our selected model, and then sum up with suggestions for future research or model improvements.


### **<a href="#top">Back to top</a>**


## Neural Networks

### About the Neural Networks

Neural Networks are information processing models that seek to learn by example. The neural network is actually inspired by the biological neuron system. One of the things that sets a neural network apart from other models is just how adaptable it is. When we say that the neural network is adaptive, what we really mean is it that it has the ability to actually modify its internal structure, and it does this by adjusting weights it assigns to inputs. A neural network in essence is an effort to bridge the gap between what a human excels at and what a machine struggles with. Essentially, the neural network tries to improve upon pattern recognition. Think of it as being able to identify what features define a dog or cat. Some machine learning programs can struggle to adapt to these changes. 

Within the neural network there are two activation functions two main types of activation functions that the neural network is capable of performing. We'll be looking into the binary functionalities of the neural network, since we're using the model for classification purposes.

### Creating the Model

After looking through a few different packages I decided that the most intuitive package for creating neural networks was the nnet library. I'll be fine tuning the values within the neural network model until I maximize the accuracy. I'll just include the final iteration model with the most optimum settings. 

```{r neuralnet}
set.seed(101)
neuralnet = nnet(status ~ usd_goal_real + backers + film + design + tech + food + music + photo + publish + art + fashion + theater + crafts + journalism + comics + dance, data = StatusTrain, size = 17, maxit = 100, decay = 0)

neuralpred <- predict(neuralnet, StatusTest)
confusion(StatusTest$status, neuralpred)
```

After tweaking key settings it was found that the optimum number of units within the hidden layer was 17. Additionally, the default maxit of 100 and decay of 0 maximized the overall accuracy to .914 (slightly lower than the KNN model). Overall, within the neural network 307 different weights were supplied. Since KNN, logistic regression, and neural network have very similar levels of accuracy, I'll run some final comparisons where I look at the sensitivity and specificity of each model in the next section. From there we'll select a final model, discuss some of the features of this model, and then proceed to discuss applications of the model, as well as using our logistic model to plot a sample project from kickstarter.

Before proceeding, I wanted to display the neural network itself via a plot. Plotting neural networks is complex, and there currently isn't a plotting function within the nnet package. Because the process of creating a plotting function for neural networks is not simple and is also not an essential goal in this research, I'm using a popular user-created function that can effectively plot neural networks. The function is known as *plot.nnet*. 

```{r neuralfunc, echo = FALSE}
plot.nnet <- function(mod.in,nid=T,all.out=T,all.in=T,bias=T,wts.only=F,rel.rsc=5,circle.cex=5,
                    node.labs=T,var.labs=T,x.lab=NULL,y.lab=NULL,line.stag=NULL,struct=NULL,cex.val=1,
                    alpha.val=1,circle.col='lightblue',pos.col='black',neg.col='grey', max.sp = F, ...){
  
  require(scales)
  
  #sanity checks
  if('mlp' %in% class(mod.in)) warning('Bias layer not applicable for rsnns object')
  if('numeric' %in% class(mod.in)){
    if(is.null(struct)) stop('Three-element vector required for struct')
    if(length(mod.in) != ((struct[1]*struct[2]+struct[2]*struct[3])+(struct[3]+struct[2])))
      stop('Incorrect length of weight matrix for given network structure')
  }
  if('train' %in% class(mod.in)){
    if('nnet' %in% class(mod.in$finalModel)){
      mod.in<-mod.in$finalModel
      warning('Using best nnet model from train output')
    }
    else stop('Only nnet method can be used with train object')
  }
  
  #gets weights for neural network, output is list
  #if rescaled argument is true, weights are returned but rescaled based on abs value
  nnet.vals<-function(mod.in,nid,rel.rsc,struct.out=struct){
    
    require(scales)
    require(reshape)
    
    if('numeric' %in% class(mod.in)){
      struct.out<-struct
      wts<-mod.in
    }
    
    #neuralnet package
    if('nn' %in% class(mod.in)){
      struct.out<-unlist(lapply(mod.in$weights[[1]],ncol))
    	struct.out<-struct.out[-length(struct.out)]
    	struct.out<-c(
    		length(mod.in$model.list$variables),
    		struct.out,
    		length(mod.in$model.list$response)
    		)    		
      wts<-unlist(mod.in$weights[[1]])   
    }
    
    #nnet package
    if('nnet' %in% class(mod.in)){
      struct.out<-mod.in$n
      wts<-mod.in$wts
    }
    
    #RSNNS package
    if('mlp' %in% class(mod.in)){
      struct.out<-c(mod.in$nInputs,mod.in$archParams$size,mod.in$nOutputs)
      hid.num<-length(struct.out)-2
      wts<-mod.in$snnsObject$getCompleteWeightMatrix()
      
      #get all input-hidden and hidden-hidden wts
      inps<-wts[grep('Input',row.names(wts)),grep('Hidden_2',colnames(wts)),drop=F]
      inps<-melt(rbind(rep(NA,ncol(inps)),inps))$value
      uni.hids<-paste0('Hidden_',1+seq(1,hid.num))
      for(i in 1:length(uni.hids)){
        if(is.na(uni.hids[i+1])) break
        tmp<-wts[grep(uni.hids[i],rownames(wts)),grep(uni.hids[i+1],colnames(wts)),drop=F]
        inps<-c(inps,melt(rbind(rep(NA,ncol(tmp)),tmp))$value)
        }
      
      #get connections from last hidden to output layers
      outs<-wts[grep(paste0('Hidden_',hid.num+1),row.names(wts)),grep('Output',colnames(wts)),drop=F]
      outs<-rbind(rep(NA,ncol(outs)),outs)
      
      #weight vector for all
      wts<-c(inps,melt(outs)$value)
      assign('bias',F,envir=environment(nnet.vals))
      }
    
    if(nid) wts<-rescale(abs(wts),c(1,rel.rsc))
    
    #convert wts to list with appropriate names 
    hid.struct<-struct.out[-c(length(struct.out))]
    row.nms<-NULL
    for(i in 1:length(hid.struct)){
      if(is.na(hid.struct[i+1])) break
      row.nms<-c(row.nms,rep(paste('hidden',i,seq(1:hid.struct[i+1])),each=1+hid.struct[i]))
    }
    row.nms<-c(
      row.nms,
      rep(paste('out',seq(1:struct.out[length(struct.out)])),each=1+struct.out[length(struct.out)-1])
      )
    out.ls<-data.frame(wts,row.nms)
    out.ls$row.nms<-factor(row.nms,levels=unique(row.nms),labels=unique(row.nms))
    out.ls<-split(out.ls$wts,f=out.ls$row.nms)
    
    assign('struct',struct.out,envir=environment(nnet.vals))
    
    out.ls
    
    }
  
  wts<-nnet.vals(mod.in,nid=F)
  
  if(wts.only) return(wts)
  
  #circle colors for input, if desired, must be two-vector list, first vector is for input layer
  if(is.list(circle.col)){
                    circle.col.inp<-circle.col[[1]]
                    circle.col<-circle.col[[2]]
                    }
  else circle.col.inp<-circle.col
  
  #initiate plotting
  x.range<-c(0,100)
  y.range<-c(0,100)
  #these are all proportions from 0-1
  if(is.null(line.stag)) line.stag<-0.011*circle.cex/2
  layer.x<-seq(0.17,0.9,length=length(struct))
  bias.x<-layer.x[-length(layer.x)]+diff(layer.x)/2
  bias.y<-0.95
  circle.cex<-circle.cex
  
  #get variable names from mod.in object
  #change to user input if supplied
  if('numeric' %in% class(mod.in)){
    x.names<-paste0(rep('X',struct[1]),seq(1:struct[1]))
    y.names<-paste0(rep('Y',struct[3]),seq(1:struct[3]))
  }
  if('mlp' %in% class(mod.in)){
    all.names<-mod.in$snnsObject$getUnitDefinitions()
    x.names<-all.names[grep('Input',all.names$unitName),'unitName']
    y.names<-all.names[grep('Output',all.names$unitName),'unitName']
  }
  if('nn' %in% class(mod.in)){
    x.names<-mod.in$model.list$variables
    y.names<-mod.in$model.list$respons
  }
  if('xNames' %in% names(mod.in)){
    x.names<-mod.in$xNames
    y.names<-attr(terms(mod.in),'factor')
    y.names<-row.names(y.names)[!row.names(y.names) %in% x.names]
  }
  if(!'xNames' %in% names(mod.in) & 'nnet' %in% class(mod.in)){
    if(is.null(mod.in$call$formula)){
      x.names<-colnames(eval(mod.in$call$x))
      y.names<-colnames(eval(mod.in$call$y))
    }
    else{
      forms<-eval(mod.in$call$formula)
      x.names<-mod.in$coefnames
      facts<-attr(terms(mod.in),'factors')
      y.check<-mod.in$fitted
      if(ncol(y.check)>1) y.names<-colnames(y.check)
      else y.names<-as.character(forms)[2]
    } 
  }
  #change variables names to user sub 
  if(!is.null(x.lab)){
    if(length(x.names) != length(x.lab)) stop('x.lab length not equal to number of input variables')
    else x.names<-x.lab
  }
  if(!is.null(y.lab)){
    if(length(y.names) != length(y.lab)) stop('y.lab length not equal to number of output variables')
    else y.names<-y.lab
  }
  
  #initiate plot
  plot(x.range,y.range,type='n',axes=F,ylab='',xlab='',...)
  
  #function for getting y locations for input, hidden, output layers
  #input is integer value from 'struct'
  get.ys<-function(lyr, max_space = max.sp){
  	if(max_space){ 
  		spacing <- diff(c(0*diff(y.range),0.9*diff(y.range)))/lyr
   	} else {
    	spacing<-diff(c(0*diff(y.range),0.9*diff(y.range)))/max(struct)
   	}
    
  		seq(0.5*(diff(y.range)+spacing*(lyr-1)),0.5*(diff(y.range)-spacing*(lyr-1)),
        length=lyr)
  }
  
  #function for plotting nodes
  #'layer' specifies which layer, integer from 'struct'
  #'x.loc' indicates x location for layer, integer from 'layer.x'
  #'layer.name' is string indicating text to put in node
  layer.points<-function(layer,x.loc,layer.name,cex=cex.val){
    x<-rep(x.loc*diff(x.range),layer)
    y<-get.ys(layer)
    points(x,y,pch=21,cex=circle.cex,col=in.col,bg=bord.col)
    if(node.labs) text(x,y,paste(layer.name,1:layer,sep=''),cex=cex.val)
    if(layer.name=='I' & var.labs) text(x-line.stag*diff(x.range),y,x.names,pos=2,cex=cex.val)      
    if(layer.name=='O' & var.labs) text(x+line.stag*diff(x.range),y,y.names,pos=4,cex=cex.val)
  }
  
  #function for plotting bias points
  #'bias.x' is vector of values for x locations
  #'bias.y' is vector for y location
  #'layer.name' is  string indicating text to put in node
  bias.points<-function(bias.x,bias.y,layer.name,cex,...){
    for(val in 1:length(bias.x)){
      points(
        diff(x.range)*bias.x[val],
        bias.y*diff(y.range),
        pch=21,col=in.col,bg=bord.col,cex=circle.cex
      )
      if(node.labs)
        text(
          diff(x.range)*bias.x[val],
          bias.y*diff(y.range),
          paste(layer.name,val,sep=''),
          cex=cex.val
        )
    }
  }
  
  #function creates lines colored by direction and width as proportion of magnitude
  #use 'all.in' argument if you want to plot connection lines for only a single input node
  layer.lines<-function(mod.in,h.layer,layer1=1,layer2=2,out.layer=F,nid,rel.rsc,all.in,pos.col,
                        neg.col,...){
    
    x0<-rep(layer.x[layer1]*diff(x.range)+line.stag*diff(x.range),struct[layer1])
    x1<-rep(layer.x[layer2]*diff(x.range)-line.stag*diff(x.range),struct[layer1])
    
    if(out.layer==T){
      
      y0<-get.ys(struct[layer1])
      y1<-rep(get.ys(struct[layer2])[h.layer],struct[layer1])
      src.str<-paste('out',h.layer)
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts<-wts[grep(src.str,names(wts))][[1]][-1]
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs<-wts.rs[grep(src.str,names(wts.rs))][[1]][-1]
      
      cols<-rep(pos.col,struct[layer1])
      cols[wts<0]<-neg.col
      
      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)
      
    }
    
    else{
      
      if(is.logical(all.in)) all.in<-h.layer
      else all.in<-which(x.names==all.in)
      
      y0<-rep(get.ys(struct[layer1])[all.in],struct[2])
      y1<-get.ys(struct[layer2])
      src.str<-paste('hidden',layer1)
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts<-unlist(lapply(wts[grep(src.str,names(wts))],function(x) x[all.in+1]))
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      wts.rs<-unlist(lapply(wts.rs[grep(src.str,names(wts.rs))],function(x) x[all.in+1]))
      
      cols<-rep(pos.col,struct[layer2])
      cols[wts<0]<-neg.col
      
      if(nid) segments(x0,y0,x1,y1,col=cols,lwd=wts.rs)
      else segments(x0,y0,x1,y1)
      
    }
    
  }
  
  bias.lines<-function(bias.x,mod.in,nid,rel.rsc,all.out,pos.col,neg.col,...){
    
    if(is.logical(all.out)) all.out<-1:struct[length(struct)]
    else all.out<-which(y.names==all.out)
    
    for(val in 1:length(bias.x)){
      
      wts<-nnet.vals(mod.in,nid=F,rel.rsc)
      wts.rs<-nnet.vals(mod.in,nid=T,rel.rsc)
      
    	if(val != length(bias.x)){
        wts<-wts[grep('out',names(wts),invert=T)]
        wts.rs<-wts.rs[grep('out',names(wts.rs),invert=T)]
    		sel.val<-grep(val,substr(names(wts.rs),8,8))
    		wts<-wts[sel.val]
    		wts.rs<-wts.rs[sel.val]
    		}
    
    	else{
        wts<-wts[grep('out',names(wts))]
        wts.rs<-wts.rs[grep('out',names(wts.rs))]
      	}
      
      cols<-rep(pos.col,length(wts))
      cols[unlist(lapply(wts,function(x) x[1]))<0]<-neg.col
      wts.rs<-unlist(lapply(wts.rs,function(x) x[1]))
      
      if(nid==F){
        wts.rs<-rep(1,struct[val+1])
        cols<-rep('black',struct[val+1])
      }
      
      if(val != length(bias.x)){
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1]),
          lwd=wts.rs,
          col=cols
        )
      }
      
      else{
        segments(
          rep(diff(x.range)*bias.x[val]+diff(x.range)*line.stag,struct[val+1]),
          rep(bias.y*diff(y.range),struct[val+1]),
          rep(diff(x.range)*layer.x[val+1]-diff(x.range)*line.stag,struct[val+1]),
          get.ys(struct[val+1])[all.out],
          lwd=wts.rs[all.out],
          col=cols[all.out]
        )
      }
      
    }
  }
  
  #use functions to plot connections between layers
  #bias lines
  if(bias) bias.lines(bias.x,mod.in,nid=nid,rel.rsc=rel.rsc,all.out=all.out,pos.col=alpha(pos.col,alpha.val),
                      neg.col=alpha(neg.col,alpha.val))
  
  #layer lines, makes use of arguments to plot all or for individual layers
  #starts with input-hidden
  #uses 'all.in' argument to plot connection lines for all input nodes or a single node
  if(is.logical(all.in)){  
    mapply(
      function(x) layer.lines(mod.in,x,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,
        all.in=all.in,pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[1]
    )
  }
  else{
    node.in<-which(x.names==all.in)
    layer.lines(mod.in,node.in,layer1=1,layer2=2,nid=nid,rel.rsc=rel.rsc,all.in=all.in,
                pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val))
  }
  #connections between hidden layers
  lays<-split(c(1,rep(2:(length(struct)-1),each=2),length(struct)),
              f=rep(1:(length(struct)-1),each=2))
  lays<-lays[-c(1,(length(struct)-1))]
  for(lay in lays){
    for(node in 1:struct[lay[1]]){
      layer.lines(mod.in,node,layer1=lay[1],layer2=lay[2],nid=nid,rel.rsc=rel.rsc,all.in=T,
                  pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val))
    }
  }
  #lines for hidden-output
  #uses 'all.out' argument to plot connection lines for all output nodes or a single node
  if(is.logical(all.out))
    mapply(
      function(x) layer.lines(mod.in,x,layer1=length(struct)-1,layer2=length(struct),out.layer=T,nid=nid,rel.rsc=rel.rsc,
                              all.in=all.in,pos.col=alpha(pos.col,alpha.val),neg.col=alpha(neg.col,alpha.val)),
      1:struct[length(struct)]
      )
  else{
    node.in<-which(y.names==all.out)
    layer.lines(mod.in,node.in,layer1=length(struct)-1,layer2=length(struct),out.layer=T,nid=nid,rel.rsc=rel.rsc,
                pos.col=pos.col,neg.col=neg.col,all.out=all.out)
  }
  
  #use functions to plot nodes
  for(i in 1:length(struct)){
    in.col<-bord.col<-circle.col
    layer.name<-'H'
    if(i==1) { layer.name<-'I'; in.col<-bord.col<-circle.col.inp}
    if(i==length(struct)) layer.name<-'O'
    layer.points(struct[i],layer.x[i],layer.name)
    }

  if(bias) bias.points(bias.x,bias.y,'B')
  
}
```

```{r plotneuralnetwork, message = FALSE, warning = FALSE, echo = FALSE}
plot.nnet(neuralnet)
```

With so many variables and a sizeable number of hidden layers (17), this is a difficult model to follow. Nevertheless, this model does showcase how complex neural networks are, and that they are adaptive models. 

### **<a href="#top">Back to top</a>**



## Model Selection

### Model Comparison

To refresh our memory, lets recall the accuracy of each of our selected models. Recall that the models we selected were the logistic regression model, the KNN model, and the neural network model.

Below is a data table that summarizes the accuracy of each model. I'll briefly talk about the accuracy of each model thus far, and then I'll review the specificity and sensitivity of each model. From there we'll make a selection for a final model, and then proceed to applying the model to a kickstarter project. 

```{r Datatable2, echo = FALSE}
Model <- c("Logistic Regression Model", "KNN (K Nearest-Neighbor) [K = 11]", "Neural Network (Size = 17)")
Overall_Accuracy <- c(89.75, 92.3, 91.4)
Summary <- data.frame(Model, Overall_Accuracy)
names(Summary) = c("Model", "Overall Accuracy (%)")
datatable(Summary)
```

Essentially, if we were to select models JUST based on accuracy, our first choice would be KNN, our second choice would be neural network, and our last choice would be the logistic regression model. However, we'll be examining the specificity and sensitivity for each model to determine what model should be our final choice. 

#### Logistic Regression Model

For the logistic regression model, I'll be creating a confusion matrix via the caret package. This will provide additional information such as specificity and sensitivity, which we'll discuss the importance of.

```{r LRMConfus}
confusionMatrix(table(Binary3, StatusTest$status), positive='1')
```

As we discussed earlier, the accuracy of our model is .8975 or 89.75%. That means the error is about 10.25%. This is a solid level of accuracy, however, while accuracy is important to consider when selecting a model, specificity and sensitivity are also just as important. 

As it turns out, our sensitivity is listed at 77.62%. Sensitivity is the proportion of target class cases that are classified correctly. This means if we were to enter some information about a project lets say the goal, an estimated number of backers, and the category there is a 77.62% chance that it will predict a successful project *as* successful. Essentially, by using this model we expect that around 23% of projects will be predicted as failed even though they in reality will succeed. This is a good ratio, but we'll take a look at specificity for our two other models to determine if this model is the most effective at predicting our target cases.

In terms of specificity, this measures the proportion of NON-target class cases that are classified correctly. With our logistic regression model our specificity was 96.58%. This is a really good percentage for specificity as it means that only around 3.4% of predict successes are false-positives. In other words, 3.4% of failed projects may be mistakenly classified as successful via the logistic regression model. While promising, we'll take a look at our statistics for the other models before making a final determination on which model we'll be selecting. Regardless, because of how intuitive the plots are for the logistic regression model we'll be running a prediction plot on our sample kickstart project we select in the conclusion section.

#### KNN (K Nearest Neighbor)

For our KNN algorithm (K nearest Neighbor), I'll construct a confusion matrix using the caret package just as I did with the logistic regression model. We'll discuss accuracy, sensitivity, and specificity just as we did before. If this model scores better in all aspects, we can eliminate the logisitic regression model, and from there choose between the KNN model and neural network model. Otherwise, we'll move onto the neural network model and then make a final determination. 

```{r KNNConfusMat}
confusionMatrix(table(knn.pred, StatusTest$status), positive='1')
```

As we observed earlier, the accuracy was .923 or 92.3%. That means the error is about 7.7%. This accuracy is slightly higher than that of the logistic model. Because it's only slightly higher in accuracy in comparison to the logistic regression model, it would be wise to take a look at the sensitivity and specificity percentages of this model to determine if its the preferable predictive model in this scenario. 

Looking at the sensitivity we can see a sizeable difference between the logistic regression model (LRM) and the KNN model. The KNN model has a sensitivity of 89.72% whereas the LRM model only has a sensitivity of 77.62%. This means the KNN model will correctly predict 89.72% of the target cases. In other words, only around 10% of successful projects will be incorrectly classified as failed. This is far more preferable than the LRM model, which incorrectly classifies 23% of successful projects as failed. But before ruling KNN as the preferable model between the two and effectively eliminated the LRM model from the running, lets take a look at the specificity percentage of the KNN model.

Interestingly enough, the specificity percentage of the KNN model is slightly lower than that of the LRM model. With a specificity rate of 93.75% the KNN model is still a great model choice. A specificity rate of 93.75% would imply that the model correctly classifies 93.75% of projects that failed. This means that only 6.25% of failed projects will be mistakenly classified as successful. Even though the LRM model incorrectly classified 3.4% of failed projects, the KNN model in my opinion is still more preferable than the LRM model. The KNN model has a slightly higher accuracy than the LRM model, and also improves immensely in terms of sensitivity, correctly identifying 89.72% of target cases. 

For the reasons discussed above, it's clear that our final model will most likely not be the logistic regression model. Now that we have *eliminated* one of the three models, we essentially only need to compare the KNN model and the Neural network model. Regardless, as stated earlier, we'll revisit the LRM model via a predictive plot for the sample project we select off the kickstarter website. 

#### Neural Network

Like with KNN and the logistic regression model, I'll attempt to create a confusion matrix via the caret package. Additionally, we'll discuss the accuracy, sensitivity, and specificity of the neural network model just as we did with the previous models. I'll be comparing the neural network to KNN model, and from there we'll make our final model selection in the next subsection. 

```{r neuralconfus}
confusionMatrix(table(neuralpred, StatusTest$status), positive='1')
```

As discussed in previous sections the overall accuracy of the neural network model is .914 or 91.4%, leaving an error of about 8.6%. It's only .9% lower than the KNN model, so we can't really determine which model is better without considering sensitivity and specificity. 

Looking at the sensitivity of the neural network model we see that it is slightly higher than the KNN model with a sensitivity of 91.14%. This means that 91.14% of target cases are successfully classified, or that only 8.86% of successful projects are mistakenly classified as failed projects. This is a mere 1.42% improvement over the KNN model. Suffice to say, it's still too close to call which model is better at this point. We'll need to move onto specificity and see if we can make any determination about which model may be better from there.

The specificity of the neural network model is .9155 or 91.55%. This means 91.55% of failed projects are correctly classified, or 8.45% of failed projects are mistakenly classified as successful. When comparing the specificity of the neural network model to that of the KNN model, we see that the KNN model has a specificity rate that is 2.2% higher than that of the neural network model. Essentially these models are still too close to call. 

Since the models are too close to call based on their accuracy, sensitivity, and specificity, we'll need to make a few considerations to come to an informed judgment. These considerations will be made in the next subsection where we'll then choose a final model. After selecting our final model, we'll explain its applications in the conclusion section, and as we've discussed we'll create a plot of a logisitic regression and apply it to a sample project currently on kickstarter. 

### Final Model Selection

Before making any considerations, lets briefly summarize the findings we discussed in the model comparison section. I've listed the accuracy, sensitivity, and specificity of each model below. Keep in mind at this point we're only deciding between the KNN model and neural network model, but will create a plot via the LRM model in the conclusion section.

```{r ModelComp2, echo = FALSE}
rm("Summary")
Model <- c("Logistic Regression Model", "KNN (K Nearest-Neighbor) [K = 11]", "Neural Network (Size = 17)")
Overall_Accuracy <- c(89.75, 92.3, 91.4)
Sensitivity <- c(77.62, 89.72, 91.14)
Specificity <- c(96.58, 93.75 ,91.55)
Summary <- data.frame(Model, Overall_Accuracy, Sensitivity, Specificity)
names(Summary) = c("Model", "Overall Accuracy (%)", "Sensitivity (%)", "Specificity (%)")
datatable(Summary)
```

**Considerations**

When deciding which model to choose between KNN and neural network, it mostly comes down to the needs for your model. Do we want a model that is only slightly less accurate, predicts target cases correctly at a slightly higher rate, and has a slightly higher chance of producing a false positive? ? Or do we want a model that is slightly more accurate, predicts target cases correctly as a slightly lower rate, and has a slightly lower chance of producing a false positive? It all comes to down to what we determine is more important to the model.

After putting some deliberation I decided the following. While predicting a successful project correctly is important, I do feel that reducing false positives is more important in this scenario. In my opinion, predicting a false positive in this scenario is more costly than failing to predict a successful project. Therefore, if we want to maintain a high level of accuracy and a high level of specificity, it would make sense to select the KNN model. 

I made this determination by thinking ahead to the applications of these models. Say for example I am an entrepreneur who is launching a kickstarter project. While running a simulation that tells me my project would fail even though in reality it would be successful may be inconvenient, a greater inconvenience would be if I based my fundraising strategy and fundraising goals partly off a simulation which falsely predicted that a failed project would be successful. Essentially, false positives can lead to potentially catastrophic results from the entrepreneurs perspective.

For this reason, I've decided that the final model I'll be using is the KNN model. In the next section I'll discuss applications of this model, and plot how the number of backers chances the probability of success for a sample project via the LRM model. 

### **<a href="#top">Back to top</a>**


## Conclusion 

### Model Applications 

After looking at some of the projects on currently live on Kickstarter, I decided to create  plot of the probabilities of success for a comic book project. The project name is [Shelter In Place Comics, vol. 1: Humor from the Year of Pandemic](https://www.kickstarter.com/projects/stannex/shelter-in-place-comics-vol-1?ref=section-comics-illustration-projectcollection-7-staff-picks-newest). As the name suggests this is basically a kickstarter project for a comic book based on the COVID-19 pandemic.

Some Key features of the Kickstarter Project for creating a plot:

* Category: Comics

* Goal in Real USD: $1,200

* Backers: We'll be plotting a range of backers from 0 to 100.

Recall that we selected the KNN model in the previous section. We'll discuss applications of the KNN model below, but to start I'll create a logistic regression plot which displays how the probability changes according to the number of backers. The reason I'll be plotting via the logistic regression model is just because it's able to generate plots that are much more intuitive. When actually running predictions, it would be wise to utilize the algorithms that the KNN model generates.

*Note: Since commencing this research project this kickstarter project surpassed its goal*

To start, I'll create a new data frame that we want to run a prediction on. I'll call this dataframe *comic*.

```{r LRMplot}
Comic <- data.frame(usd_goal_real = 1200, backers = 0:200, design = 0, film = 0, tech = 0, food = 0, music = 0, photo = 0, publish = 0, art = 0, fashion = 0, theater = 0, crafts = 0, journalism = 0, comics = 1, dance = 0)
```

```{r LRMPlot, echo = FALSE}
Logit_Sim2 <- predict(LRM, Comic, type = "response")

plot(0:200, Logit_Sim2, type = "l", lwd = 1, col = "red", xlab = "Backers", ylab = "Probability", main = "Probability of Success, Comic Project with $1,200 Goal")
rm("neuralnet", "neuralpred")
```

Recall, we decided that of our three final models the LRM model was actually least preferable. But as you can see, with the LRM model we do receive a much more intuitive plot than what we would receive from the KNN and neural network plots. As you can see, the model predicts that the probability of success seems to reach 90% once the project garners around 75 backers. 

**Applications of KNN**

We can think of KNN almost as an algorithm that is used in situations where you may want to *recommend* a course of action based on the KNN algorithm's ability to identify similar observations. This is a promising model with a lot of benefits, but as we saw earlier KNN is not a *fast* algorithm. Recall that when we were initially running the KNN algorithm and substituting k-values, we opted only to include the relevant code for the optimum k-value to reduce knitting time. As the volume of the data increases, so too does our KNN processing time. With our dataset numbering in the range of over 350,000 observations, this slows down our processing speed immensely. Suffice to say we would not use the KNN algorithm for recommendation platforms which require fast processing times. Take for example Netlfix, where the algorithm will sort through millions of observations and quickly recommend titles for a user to watch. Such a model will not be feasible, and a more complex alogrithm that generates results faster would be preferable. *However*, for our application the KNN algorithm albeit  much slower with 350,000 observations, will work just fine. 

**Utilizing KNN for Kickstarter**

If I was entrepreneur trying to get an idea of the amount of backers (which essentially gives us an average pledge amount) I would need for my project to succeed, I could effectively expand our test set for KNN, and depending on what category my project is in filter all my observations to just that category. From there I can develop some benchmarks, for example maybe I can decide on the minimum number of backers I would need, and then set my pledge levels from there. 

To demonstrate, here's the KNN predictions we received earlier from our KNN at k = 11. I merged the predictions with the corresponding test data set and filtered the data just to comic projects to demonstrate how the KNN algorithm can predict if a project is successful based on differing goals and backers.

```{r knnplot, echo = FALSE}
KNNComic <- data.frame(knn.pred, KNNTest)
KNNComic <- filter(KNNComic, KNNComic$StatusTest.comics == 1)
KNNComic <- data.frame(KNNComic$knn.pred, KNNComic$StatusTest.usd_goal_real, KNNComic$StatusTest.backers)
names(KNNComic) = c("Predicted Outcome", "Goal (Real USD)", "Backers")
datatable(KNNComic)
```

As you may know, within kickstarter you are able to set pledge levels. Within any project there is a base pledge level, and as the amount pledged increases backers are able to reach higher tier pledge levels (which come with extra perks should the project succeed). You could use the algorithm that KNN provides to determine the minimum number backers you would need to have a high probability of success. From there, you could determine the best pledge levels, adjust your pitching strategy, and ensure your goal is reasonable. 

Kickstarter may even utilize such a algorithm. It could very well be that some projects are at at disadvantage solely because of the goals they may set and the category the project belongs to. Kickstarter could potentially expand upon such an algorithm (it may not be KNN necessarily) to boost interaction with projects that may have a slight disadvantage so that essentially all projects start off at a level playing field. 

Despite some of the uses of KNN, there's ways that we could potentially improve our KNN model in the future.


**Ways to Improve our KNN Model**

Assuming we were to create more variables and source more data there are ways we could potentially improve our KNN algorithm. Note that kickstarter projects normally have a risk section, where they discuss any risks that may be present in the venture. Maybe constructing a risk index and implementing it across kickstarter projects could prove to be beneficial and further improve accuracy. It could very well be that high levels of risk may dissuade users from investing in a project. If we can capture this measure it could potentially improve our model. 

Additionally, although backers are important to project success, it would be interesting to create a model that incorporates the number of interactions with the project page. To add onto that, we could determine the average number of interactions a page receives for each backer. By doing this entrepreneurs can strategize how to drive page traffic to potentially increase the number of backers for their projects. 

Lastly, for more specific predictions it would be interesting to create a algorithm that predicts success or failure based off of the subcategory for the project. Take for example the comic book project we took a brief look at. Maybe we could predict more accurate outcomes if we instead ran our KNN algorithm based on the subcategory of the project. Maybe certain subcategories perform better than others within the comic category. Although not intially part of the research goals, this could be an interesting model to create in the future. 

### A Summary of Findings

Throughout this research process we came across several interesting insights. For one, we realized that some categories see much higher rates of success than others. Furthermore, when generating our models we were able to observe how differing average success rates across categories lead to differing coefficient estimates. Because of this, the category a aspiring entrepreneur launches their project under may actually lead to a slight advantage or disadvantage. It was even found that some categories had higher success rates than failure rates within our dataset.

In addition to our findings about how project categories may effect the potential success of a project, we also examined just how spread out the distribution of project goals are. Some projects may have goals that are just a few hundred dollars, and some projects may have goals in the millions of dollars. We saw via our models (specifically our logistic regression model) that as the dollar amount of the goal increases, the probability of success decreases. This can add up and causes projects with high goals on kickstarter to become unsuccessful despite the fact that the idea may be innovative and frankly worth the investment.

While intuitive, our models showed us that as the number of backers increases the probability of success increases. Rather than just assigning values to the backer variable however, we opted to run predictions using a range of backers instead. It was decided that by doing this we can see what the minimum number of backers would be to ensure a high probability of success, and from there we could create more informed pitch strategies, set appropriate pledge levels, and ensure our goal seems reasonable given how many backers we may expect.   

### Recommendations for Improving the Dataset

#### Creating Models off of Keywords

Although this may take much more time and is a complex process, it would be interesting to see if certain keywords cause a project to be more likely to fail or to be successful. Specifically, it would be interesting to see if keywords within the description of the kickstarter project can influence the outcome of the project. Maybe certain words gain more traction and can trigger more traffic on the project page. 

#### Creating a Risk Index

As discussed earlier, creating a risk index would be extremely useful for future models. It can measure how perceived risk of project effects the project's traction. It could very well be the case that risk may not have as big of an effect on hitting project goals as one might think. However, sourcing data which ranks the risk for kickstarter project and incorporating that data into our models would most certainly be insightful.

#### Experience

Maybe the industry experience of the project's team influences how willing individuals may be to fund a project. We could collect data on this and incorporate the data into a model to determine how much of an effect professional experience may have on a Kickstarter's success. It could very well be that hobbyists are given the same "*benefit of the doubt*" that professionals are within the platform. 

#### Subcategories

As discussed in a prior subsection, we could create a separate model that instead predicts success based on subcategories. Assuming we're an entrepreneur whose about to launch a project it would be helpful to know what the playing fields are. We would not just want to know what the playing field is based on the category, but the subcategory as well. 

#### Page Interactions

Page interactions would be a unique variable that could be included in future model creations. We can answer questions such as the following:

* How does a single page interaction increase the chance of a projects success?

### A Note For Future Model Creations

#### Adding More Variables and Potentially Removing the Backer Variable. 

After adding more relevant variables to the model, it would be preferable if eventually we are able to remove the backers variable, or create a separate model which doesn't include backers. Note that this is really just a base model for predicting if a project will be successful, and as a base model the variable *backers* works perfectly fine. But from an entrepreneurial perspective, I feel that I would ultimately want a model that can better help me with a pitch strategy. As entrepreneur, the ability to effectively pitch your product is absolutely essential. I would want to know how much of a concern my experience is to potential backers, how much of a concern is the project risk, how much impact do certain keywords have on my project's success.

This would obviously require a much more complex model with variables currently not present in our dataset. However, if I could know just how much perceived risk effects the chances of my projects success, how experience effects a potential backer's perception of my project, and how keywords can convert to more page traction or even potential pledges, I can create a pitch that is fine tuned to meet my project's specific needs. 

#### Using Added Variables to Improve the Logistic Regression Model

One of the features that I think is particularly useful about the logistic regression model is that it provides a probability of success rather than simply providing a 0 (failure) or 1 (success). Should we improve the dataset by adding more variables, I would prioritize trying to improve the logistic regression model by implementing whichever added variable seems significant. If we could improve the sensitivity and accuracy of the logistic regression model so that it is at least closer to the rates of the KNN model, as an entrepreneur I'd be more inclined to use the logistic regression model. 

### **<a href="#top">Back to top</a>**


